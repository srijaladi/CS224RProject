import torchimport torch.nn as nnimport randomimport numpy as npimport DQNBasefrom DQNBase import DQNimport State2048Filefrom State2048File import State2048import Conv_Statefrom Conv_State import CONV_STATETOTAL_SQUARES = 16TOTAL_OPTIONS = 20TOTAL_ACTIONS = 4SEED = 42np.random.seed(SEED)random.seed(SEED)torch.random.manual_seed(SEED)torch.manual_seed(SEED)GOAL = 9def reward_function(state, non_term, valid_move):    if not(valid_move) or not(non_term):        return -100    elif np.amax(state) < 9:        return -0.2    else:        return 100    iterations = 1000SAMPLE_SIZE = 64start_lr = 1e-4epsilon = 1GAMMA = 0.9model_save_freq = 100eps_decay_rate = 0.8state_version = "base"state_conv = {"old" : 4 * 4 * TOTAL_OPTIONS, "base" : 4 * 4 * 2 * 3, "each" : 4 * 4 * 5, "alt" : 4}ENV = State2048("Log", reward_function)ENCODER = CONV_STATE(state_version, ENV)IN_DIM = int(state_conv[state_version])OUT_DIM = TOTAL_ACTIONSlayers = [nn.Linear(IN_DIM,128), nn.ReLU(), nn.Linear(128,64), nn.ReLU(), nn.Linear(64,32), nn.ReLU(), nn.Linear(32,16), nn.ReLU(), nn.Linear(16,8), nn.ReLU(), nn.Linear(8,OUT_DIM)]DQNetwork = DQN(layers, start_lr, TOTAL_ACTIONS)DQNetwork.load_state_dict(torch.load("model_params.txt"))def greedy_action(model, state):    action_values = model(state)    action = torch.argmax(action_values)    return int(action), action_values[action]def greedy_action_full(all_models, state):    action_values = np.zeros(4)    for action in range(4):        action_values[action] = all_models[action](state)    action = np.argmax(action_values)    return int(action), action_values[action]def buffer_train(D, sample_size, g, model):    sample = random.sample(D, min(len(D),sample_size))    loss = 0    for s,a,r,s_next,n_t,v_m in sample:        tv = r        if n_t and v_m: tv += g * greedy_action(model, s_next)[1].item()        loss += model.backprop_single(s, a, tv)    avg_loss = loss/len(sample)    return avg_lossvalid_move_rate = []action_freq = np.zeros(4)avg_losses = []total_moves = []total_score = []highest_tile = []D = []upto = 100allowed_moves = 300for iteration in range(1,iterations+1):    moves = 0    real_moves = 0    valid_moves = 0    non_term, valid_move = True, True    #state_np = ENV.initialize_state()    state_np = np.zeros((4,4))    state_np[0][0], state_np[0][1] = 1, 1    loss = 0    trains = 0        epsilon *= eps_decay_rate        avg_loss = 0    avg_loss_count = 0        """    if iteration > upto:        allowed_moves += 100        upto += allowed_moves    """        while non_term and valid_move and moves < allowed_moves and np.amax(state_np) < GOAL:        state = ENCODER.convert(state_np)                action = -1        rand = False        if random.uniform(0,1) <= epsilon:            action = random.choice([0,1,2,3])            rand = True        else:            action, predicted_action_value = greedy_action(DQNetwork, state)                state_next_np, non_term, valid_move = ENV.transition(state_np, action)        state_next = ENCODER.convert(state_next_np)        reward = ENV.state_score(state_next_np, non_term, valid_move)                if not(rand): real_moves += 1        moves += 1        valid_moves += (not(rand) and valid_move)        if not(rand): action_freq[action] += 1                D.append((state,action,reward,state_next,non_term,valid_move))                _, target_action_value = greedy_action(DQNetwork, state_next)        target_val = reward + GAMMA * target_action_value.item()        if not(valid_move) or not(non_term): target_val = reward        DQNetwork.backprop_single(state, action, target_val)        poss = ENV.possible_actions(state_np)        if len(poss) < 4:            trains += 1            for act in range(4):                t1, t2, t3 = ENV.transition(state_np, act)                rew = ENV.state_score(t1,t2,t3)                D.append((state,act,rew,ENCODER.convert(t1),t2,t3))                if not(t2) or not(t3):                    DQNetwork.backprop_single(state,act,rew)                avg_loss += buffer_train(D, SAMPLE_SIZE, GAMMA, DQNetwork)        avg_loss_count += 1                state_np = np.copy(state_next_np)            if iteration%100 == 0:        print(iteration)            if iteration%model_save_freq:        torch.save(DQNetwork.state_dict(), "model_params.txt")            if avg_loss_count: avg_loss /= avg_loss_count    avg_losses.append(avg_loss)        if real_moves: valid_move_rate.append(valid_moves/real_moves)    else: valid_move_rate.append(0)    total_moves.append(moves)    total_score.append(np.sum(state_np))    highest_tile.append(int(2 ** (np.amax(state_np))))    np.savetxt("valid_move_rate.txt", np.array(valid_move_rate))    np.savetxt("action_freq.txt", action_freq)    np.savetxt("avg_losses.txt", np.array(avg_losses))    np.savetxt("total_moves.txt", np.array(total_moves))    np.savetxt("total_score.txt", np.array(total_score))    np.savetxt("highest_tile.txt", np.array(highest_tile))              