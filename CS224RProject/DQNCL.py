import torchimport torch.nn as nnimport randomimport numpy as npimport DQNBasefrom DQNBase import DQNimport State2048Filefrom State2048File import State2048import Conv_Statefrom Conv_State import CONV_STATEFOLDER_PREFIX = "DQNCL/"DEVICE = "cuda" if torch.cuda.is_available() else "cpu"print(DEVICE)TOTAL_SQUARES = 16TOTAL_OPTIONS = 20TOTAL_ACTIONS = 4SEED = 42np.random.seed(SEED)random.seed(SEED)torch.random.manual_seed(SEED)torch.manual_seed(SEED)def reward_function(prev_state, state):    return -1    iterations_per = 250SAMPLE_SIZE = 64start_lr = 1e-4GAMMA = 0.9model_save_freq = 100state_version = "old"state_conv = {"old" : 4 * 4 * TOTAL_OPTIONS, "base" : 4 * 4 * 2 * 3, "each" : 4 * 4 * 5, "alt" : 4}ENV = State2048("Log", reward_function)ENCODER = CONV_STATE(state_version, ENV)OUT_DIM = TOTAL_ACTIONSlayers = [torch.nn.Conv2d(TOTAL_OPTIONS, 256, kernel_size = (2, 2)), torch.nn.ReLU(),          torch.nn.Conv2d(256, 128, kernel_size = (2, 2)), torch.nn.ReLU(),          torch.nn.Flatten(start_dim = 0), torch.nn.Linear(128 * 2 * 2, 128),           torch.nn.ReLU(), torch.nn.Linear(128,OUT_DIM)]DQNetwork = DQN(layers, start_lr, TOTAL_ACTIONS).to(DEVICE)def greedy_action(model, state, poss_actions):    action_values = model(state)    best_value, best_action = -10 ** 10, -1    for action in poss_actions:        value = action_values[action]        if value > best_value:            best_value, best_action = value, action    return int(best_action), best_valuedef buffer_train(D, episode_samples, g, model):    sample = random.sample(D, min(len(D),episode_samples))    loss = 0    for episode in sample:        for s,a,r,s_next,n_t,v_m,pa in episode[::-1]:            tv = r            if n_t and v_m: tv += g * greedy_action(model, s_next, pa)[1].item()            loss += model.backprop_single(s, a, tv, DEVICE)    avg_loss = loss/len(sample)    return avg_lossaction_freq = np.zeros(4)avg_losses = []total_moves = []total_score = []highest_tile = []D = []allowed_moves = 500for target in range(7,12):    best = 0    for iteration in range(1,iterations_per+1):        moves, trains, loss = 0, 0, 0        non_term, valid_move = True, True                        state_np = ENV.initialize_state()                episode = []                while non_term and moves < allowed_moves and np.amax(state_np) < target:            state = ENCODER.convert(state_np).to(DEVICE)                         poss_actions = ENV.possible_actions(state_np)            action, prediced_action_value = greedy_action(DQNetwork, state, poss_actions)                        state_next_np, non_term, valid_move = ENV.transition(state_np, action)            non_term = ENV.check_move_is_possible(state_next_np)                        state_next = ENCODER.convert(state_next_np).to(DEVICE)            reward = ENV.combined                        moves, action_freq[action] = moves + 1, action_freq[action] + 1                        episode.append((state,action,reward,state_next,non_term,valid_move,poss_actions))                        state_np = np.copy(state_next_np)                    D.append(episode)                avg_loss = buffer_train(D, SAMPLE_SIZE, GAMMA, DQNetwork)                    ######################################################        ######################################################        ######################################################        #################### PURE LOGGING ####################        best = max(best, int(2 ** (np.amax(state_np))))        if iteration%10 == 0:            print(iteration, best)            best = 0                    if iteration%model_save_freq:            torch.save(DQNetwork.state_dict(), FOLDER_PREFIX + "model_params.txt")                    avg_losses.append(avg_loss)        total_moves.append(moves)        total_score.append(np.sum(state_np))        highest_tile.append(int(2 ** (np.amax(state_np))))                np.savetxt(FOLDER_PREFIX + "action_freq.txt", action_freq)        np.savetxt(FOLDER_PREFIX + "avg_losses.txt", np.array(avg_losses))        np.savetxt(FOLDER_PREFIX + "total_moves.txt", np.array(total_moves))        np.savetxt(FOLDER_PREFIX + "total_score.txt", np.array(total_score))        np.savetxt(FOLDER_PREFIX + "highest_tile.txt", np.array(highest_tile))          #################### PURE LOGGING ####################        ######################################################        ######################################################        ######################################################            