import torchimport torch.nn as nnimport randomimport numpy as npDOUBLE_DQN = Falseclass Game2048:    def __init__(self,version):        self.version = version        if self.version == "Log":            self.base = 1        elif self.version == "Reg":            self.base = 2        else:            print("VERSION MUST BE EITHER 'REG' OR 'LOG'")        self.state = np.zeros((4,4))        self.score = 0        self.restart()        self.dr4 = [-1,0,1,0]        self.dc4 = [1,0,-1,0]        self.score = 0            def restart(self):        self.state = np.zeros((4,4))        self.sample_n_locations(2)            def gen_new_number(self):        z = random.uniform(0,1)        if z > 0.1:            return self.base        else:            return self.increment_number(self.base, False)            def increment_number(self, num, actual):        if self.version == "Reg":            if actual: self.score += num * 2            return num * 2        elif self.version == "Log":            if actual: self.score += num + 1            return num + 1        else:            print("VERSION MUST BE EITHER 'REG' OR 'LOG'")            def move_by_action(self, action: int, actual: bool):        curr_state = np.copy(self.state)        done = [[0 for _ in range(4)] for _ in range(4)]        combines = 0        combined_amt = 0        if action == 0:            for c in range(4):                for r in range(1,4):                    num = curr_state[r,c]                    if num == 0: continue                    found = -1                    for j in range(r-1,-1,-1):                        if curr_state[j,c] != 0:                            found = j                            break                    if found < 0 or curr_state[found,c] != num or done[found][c]:                        curr_state[r,c] = 0                        curr_state[found+1,c] = num                    else:                        curr_state[r,c] = 0                        curr_state[found,c] = self.increment_number(curr_state[found,c], actual)                        combined_amt += curr_state[found,c]                        combines += 1                        done[found][c] = 1        elif action == 1:            for r in range(4):                for c in range(2,-1,-1):                    num = curr_state[r,c]                    if num == 0: continue                    found = 4                    for j in range(c+1,4):                        if curr_state[r,j] != 0:                            found = j                            break                    if found > 3 or curr_state[r,found] != num or done[r][found]:                        curr_state[r,c] = 0                        curr_state[r,found-1] = num                    else:                        curr_state[r,c] = 0                        curr_state[r,found] = self.increment_number(curr_state[r,found], actual)                        combined_amt += curr_state[found,c]                        combines += 1                        done[r][found] = 1        elif action == 2:            for c in range(4):                for r in range(2,-1,-1):                    num = curr_state[r,c]                    if num == 0: continue                    found = 4                    for j in range(r+1,4):                        if curr_state[j,c] != 0:                            found = j                            break                    if found > 3 or curr_state[found,c] != num or done[found][c]:                        curr_state[r,c] = 0                        curr_state[found-1,c] = num                    else:                        curr_state[r,c] = 0                        curr_state[found,c] = self.increment_number(curr_state[found,c], actual)                        combined_amt += curr_state[found,c]                        combines += 1                        done[found][c] = 1        elif action == 3:            for r in range(4):                for c in range(1,4):                    num = curr_state[r,c]                    if num == 0: continue                    found = -1                    for j in range(c-1,-1,-1):                        if curr_state[r,j] != 0:                            found = j                            break                    if found < 0 or curr_state[r,found] != num or done[r][found]:                        curr_state[r,c] = 0                        curr_state[r,found+1] = num                    else:                        curr_state[r,c] = 0                        curr_state[r,found] = self.increment_number(curr_state[r,found], actual)                        combined_amt += curr_state[found,c]                        combines += 1                        done[r][found] = 1        else:              print("ERROR INCORRECT INPUT ACTION")                return curr_state, combines, combined_amt        def eval_state(self, state):        all_list = sorted([[-state[r][c],r,c] for r in range(4) for c in range(4)])        base = [4,3,2,1]        mult = [8,6,4,2]        evaluation = 0        for index in range(4):            start = base[index]            row = all_list[index][1]            col = all_list[index][2]            if row > 0:                dist = 20            elif abs(col - index) > 2:                dist = 2            else:                dist = abs(col - index)            add_eval = start - dist * mult[index]            evaluation += (add_eval * abs(all_list[index][0]))                open_squares = np.sum(state == 0)        evaluation += open_squares * 50                counts = [0 for _ in range(20)]                for row in state:            for val in row:                counts[int(val)] += 1                        duplicate_penalty = 0        dupe_pen_const = 10        for val in range(1,len(counts)):            duplicate_penalty += (dupe_pen_const * (counts[val]-1) * val)                    evaluation -= duplicate_penalty                return evaluation        def transition(self, action: int):        self.state, combined_count, combined_amt = self.move_by_action(action,True)                    curr_eval = self.eval_state(self.state)                if not(self.check_move_is_possible()):            return self.state, -2000, False        else:            return self.state, -10 + 10 * combined_amt + curr_eval, True            def score_board(self):        return np.sum(self.state)            def check_move_is_possible(self):        for r in range(4):            for c in range(4):                if self.state[r][c] == 0:                    return True                if r+1 < 4 and self.state[r+1][c] == self.state[r][c]:                    return True                if c+1 < 4 and self.state[r][c+1] == self.state[r][c]:                    return True        return False            def sample_n_locations(self, sample_count: int):        locations = [(i,j) for i in range(4) for j in range(4) if self.state[i,j] == 0]        if len(locations) == 0: return False        for sample_r, sample_c in random.sample(locations, sample_count):            self.state[sample_r,sample_c] = self.gen_new_number()        return True        def print_state(self):        for r in range(4):            for c in range(4):                print(int(self.state[r][c]), end = " ")            print()                def possible_moves(self):        possible = []        for a in range(4):            if np.array_equal(self.state, self.move_by_action(a, False)[0]):                continue            possible.append(a)        return possibleTOTAL_SQUARES = 16TOTAL_OPTIONS = 20TOTAL_ACTIONS = 4LRStart = 1e-2GAMMA = 0.9class DQN(nn.Module):    def __init__(self):        super().__init__()        layers = [nn.Linear(TOTAL_SQUARES * TOTAL_OPTIONS,128), nn.ReLU(),                   nn.Linear(128,64), nn.ReLU(),                  nn.Linear(64,32), nn.ReLU(),                  nn.Linear(32,16), nn.ReLU(),                   nn.Linear(16,8), nn.ReLU(),                  nn.Linear(8,4)]        self.model = nn.Sequential(*layers)        self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = LRStart)            def forward(self, input_state):        return self.model(input_state)        def backprop_single(self, state, action, true_value):        self.optimizer.zero_grad()        computed_value = torch.sum(self.model(state) * torch.nn.functional.one_hot(torch.tensor(action).to(torch.int64), TOTAL_ACTIONS))        #mse_loss = torch.nn.functional.mse_loss(true_value, computed_value)        mse_loss = (computed_value - true_value) ** 2        mse_loss.backward()        self.optimizer.step()            def backprop_batch(self, states, actions, true_values):        self.optimizer.zero_grad()        mask = torch.nn.functional.one_hot(actions.to(torch.int64), TOTAL_ACTIONS)        #print(mask.size())        #print(states.size())        #print(true_values.size())        #print()        values = torch.sum(self.model(states) * mask, dim = 1)        mse_loss = torch.nn.functional.mse_loss(true_values.float(), values.float())        mse_loss.backward()        self.optimizer.step()        epsilon = 1iterations = 10000minibatch_size = 10copy_model_freq = 10decrement_lr_freq = 100model_save_freq = 100decrement_eps_freq = 50eps_decay_rate = 0.8DQNetwork = DQN()TargetNetwork = DQN()score_history = np.array([])best_square_history = np.array([])move_history = np.array([])#print(DQNetwork.state_dict())D = []def generate_greedy_action(network, state, possible_actions, DoubleDQN = False, printer = False, other_network = None):    action_values = network(state)    if DoubleDQN: action_values = other_network(state)    mask = torch.zeros(TOTAL_ACTIONS)    base_value = 1000000    mask[possible_actions] = base_value    action = torch.argmax(action_values + mask)    if DoubleDQN: return network(state)[action]    if printer: print(action_values)    return int(action), action_values[action]def conv_state(state_np):    return torch.flatten(torch.nn.functional.one_hot(torch.tensor(state_np).to(torch.int64), TOTAL_OPTIONS)).float()for iteration in range(1,iterations+1):    Game = Game2048("Log")        moves = 0    real_moves = 0    going = True    while going:        curr_state_np = np.copy(Game.state)        possible_actions = Game.possible_moves()        state = conv_state(curr_state_np)                action = -1        rand = False        if random.uniform(0,1) <= epsilon:            action = random.choice(possible_actions)            rand = True        else:            action, predicted_action_value = generate_greedy_action(DQNetwork, state, possible_actions, False)                state_next_np, reward, going = Game.transition(action)        new_poss_actions = Game.possible_moves()                if not(rand): real_moves += 1        moves += 1                state_next = conv_state(state_next_np)                        D.append((state,action,reward,state_next,new_poss_actions, going))                samples = random.sample(D, min(len(D),minibatch_size))        for s,a,r,s_next,poss_actions,not_term in samples:            _, target_action_value = generate_greedy_action(TargetNetwork, s_next, poss_actions)            target_val = r + GAMMA * target_action_value.item()            if DOUBLE_DQN:                if np.random.uniform(0,1) < 0.5:                    TargetNetwork.backprop_single(s, a, r + GAMMA * generate_greedy_action(TargetNetwork, s_next, poss_actions, True, False, DQNetwork).item())                else:                    DQNetwork.backprop_single(s, a, r + GAMMA * generate_greedy_action(DQNetwork, s_next, poss_actions, True, False, DQNetwork).item())            else:                if not_term:                    DQNetwork.backprop_single(s, a, target_val)                else:                    DQNetwork.backprop_single(s, a, r)        if not(going) and False:            for i in range(minibatch_size):                DQNetwork.backprop_single(state, action, reward)        ##### SAVING MODEL INFO #####    if iteration%copy_model_freq == 0:        TargetNetwork.model.load_state_dict(DQNetwork.model.state_dict())    if iteration%decrement_eps_freq == 0:        epsilon -= 0.01    if iteration%decrement_lr_freq == 0:        for g in DQNetwork.optimizer.param_groups:            g['lr'] *= 0.9        achieved_score = Game.score    max_val = 2 ** int(np.max(Game.state))        score_history = np.append(score_history, achieved_score)    best_square_history = np.append(best_square_history, max_val)    move_history = np.append(move_history, real_moves)        print(iteration, achieved_score, max_val, real_moves, moves)    if iteration%model_save_freq == 0:        torch.save(DQNetwork.state_dict(), "model_params.txt")    if iteration%10 == 0:        print("ITERATION: " + str(iteration) + " COMPLETED WITH LOG SCORE " + str(achieved_score) + " AND MAX SQUARE " + str(max_val))        print(Game.state)        print()            if iteration%10 == 0:        np.savetxt("score_history.txt", score_history)        np.savetxt("best_square_history.txt", best_square_history)        np.savetxt("move_history.txt", move_history)    ##### SAVING MODEL INFO #####